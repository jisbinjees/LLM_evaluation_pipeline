{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a034a9e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60ea935751d4544842c49e49c5ecc15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JISBIN JEES\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\JISBIN JEES\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9c1dfe53be4740b442c7b3c7161740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e515a0d41e47f8bb30f12355f0b275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71d595186494d7995851bcce40300b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d45d5143d034fd6b19b05f43ccf5797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63f59ede4d849aa901ae56f0ae4df78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ff66bfdd0d47dfad56a86f4795b06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe785e7488b4d8a948469d6d73f9827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d46a2ecd4d48d387276494718a5079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7d80806ac647a38371613a7639b052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129b8c9325ac463984c6b7bb7aba131c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"relevance_score\": 0.718,\n",
      "  \"hallucination_score\": 0.24,\n",
      "  \"latency_ms\": 3309,\n",
      "  \"final_decision\": \"FAIL\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LLM Response Evaluation Pipeline\n",
    "\n",
    "Evaluates AI responses against:\n",
    "1. Response Relevance & Completeness\n",
    "2. Hallucination / Factual Accuracy\n",
    "3. Latency\n",
    "\n",
    "Compatible with:\n",
    "- sample-chat-conversation-01.json\n",
    "- sample_context_vectors-01.json\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class LLMEvaluationPipeline:\n",
    "    \"\"\"End-to-end evaluation pipeline for LLM responses.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "        relevance_threshold: float = 0.75,\n",
    "        hallucination_threshold: float = 0.30\n",
    "    ) -> None:\n",
    "        self.model = SentenceTransformer(embedding_model)\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        self.hallucination_threshold = hallucination_threshold\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Utility methods\n",
    "    # ------------------------------------------------------------------\n",
    "    def _embed(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Generate normalized embeddings.\"\"\"\n",
    "        return self.model.encode(\n",
    "            texts,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_last_user_and_ai_messages(\n",
    "        conversation_json: Dict\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Extracts the latest user message and its corresponding AI reply.\n",
    "        \"\"\"\n",
    "        user_message = \"\"\n",
    "        ai_message = \"\"\n",
    "\n",
    "        for turn in reversed(conversation_json.get(\"conversation_turns\", [])):\n",
    "            if not ai_message and turn.get(\"role\") == \"AI/Chatbot\":\n",
    "                ai_message = turn.get(\"message\", \"\")\n",
    "            elif ai_message and turn.get(\"role\") == \"User\":\n",
    "                user_message = turn.get(\"message\", \"\")\n",
    "                break\n",
    "\n",
    "        return {\n",
    "            \"user_message\": user_message,\n",
    "            \"ai_message\": ai_message\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_context_texts(context_json: Dict) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extracts all retrieved context chunks from vector DB response.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            vector.get(\"text\", \"\")\n",
    "            for vector in context_json\n",
    "            .get(\"data\", {})\n",
    "            .get(\"vector_data\", [])\n",
    "            if vector.get(\"text\")\n",
    "        ]\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Evaluation metrics\n",
    "    # ------------------------------------------------------------------\n",
    "    def compute_relevance(\n",
    "        self,\n",
    "        user_message: str,\n",
    "        ai_message: str\n",
    "    ) -> float:\n",
    "        \"\"\"Computes semantic relevance between user query and AI response.\"\"\"\n",
    "        embeddings = self._embed([user_message, ai_message])\n",
    "        score = cosine_similarity(\n",
    "            [embeddings[0]],\n",
    "            [embeddings[1]]\n",
    "        )[0][0]\n",
    "        return round(float(score), 3)\n",
    "\n",
    "    def compute_hallucination(\n",
    "        self,\n",
    "        ai_message: str,\n",
    "        context_texts: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Computes hallucination score.\n",
    "        Higher score => higher hallucination risk.\n",
    "        \"\"\"\n",
    "        if not context_texts:\n",
    "            return 1.0\n",
    "\n",
    "        ai_embedding = self._embed([ai_message])[0]\n",
    "        context_embeddings = self._embed(context_texts)\n",
    "\n",
    "        similarities = cosine_similarity(\n",
    "            [ai_embedding],\n",
    "            context_embeddings\n",
    "        )[0]\n",
    "\n",
    "        max_similarity = float(np.max(similarities))\n",
    "        hallucination_score = 1 - max_similarity\n",
    "\n",
    "        return round(hallucination_score, 3)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_latency(start_time: float) -> int:\n",
    "        \"\"\"Returns latency in milliseconds.\"\"\"\n",
    "        return int((time.time() - start_time) * 1000)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Main evaluation\n",
    "    # ------------------------------------------------------------------\n",
    "    def evaluate(\n",
    "        self,\n",
    "        conversation_json: Dict,\n",
    "        context_json: Dict\n",
    "    ) -> Dict:\n",
    "        \"\"\"Runs the full evaluation pipeline.\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        messages = self._extract_last_user_and_ai_messages(\n",
    "            conversation_json\n",
    "        )\n",
    "        context_texts = self._extract_context_texts(context_json)\n",
    "\n",
    "        relevance_score = self.compute_relevance(\n",
    "            messages[\"user_message\"],\n",
    "            messages[\"ai_message\"]\n",
    "        )\n",
    "\n",
    "        hallucination_score = self.compute_hallucination(\n",
    "            messages[\"ai_message\"],\n",
    "            context_texts\n",
    "        )\n",
    "\n",
    "        latency_ms = self.compute_latency(start_time)\n",
    "\n",
    "        final_decision = (\n",
    "            \"PASS\"\n",
    "            if relevance_score >= self.relevance_threshold\n",
    "            and hallucination_score <= self.hallucination_threshold\n",
    "            else \"FAIL\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"relevance_score\": relevance_score,\n",
    "            \"hallucination_score\": hallucination_score,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"final_decision\": final_decision\n",
    "        }\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Script entry point\n",
    "# ----------------------------------------------------------------------\n",
    "def load_json(file_path: str) -> Dict:\n",
    "    \"\"\"Loads a JSON file safely.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    conversation_json = load_json(\"sample-chat-conversation-01.json\")\n",
    "    context_json = load_json(\"sample_context_vectors-01.json\")\n",
    "\n",
    "    evaluator = LLMEvaluationPipeline()\n",
    "    result = evaluator.evaluate(conversation_json, context_json)\n",
    "\n",
    "    print(json.dumps(result, indent=2))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426f9de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence_transformers in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from sentence_transformers) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.24.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from sentence_transformers) (0.36.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.46.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from sentence_transformers) (10.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from sentence_transformers) (4.59.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from sentence_transformers) (2.4.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: requests in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.8)\n",
      "Requirement already satisfied: networkx in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (2.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.22.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2021.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from networkx->torch>=1.11.0->sentence_transformers) (5.0.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2020.12.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jisbin jees\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ccef81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
